{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13175 , -0.25517 , -0.067915,  0.26193 , -0.26155 ,  0.23569 ,\n",
       "        0.13077 , -0.011801,  1.7659  ,  0.20781 ,  0.26198 , -0.16428 ,\n",
       "       -0.84642 ,  0.020094,  0.070176,  0.39778 ,  0.15278 , -0.20213 ,\n",
       "       -1.6184  , -0.54327 , -0.17856 ,  0.53894 ,  0.49868 , -0.10171 ,\n",
       "        0.66265 , -1.7051  ,  0.057193, -0.32405 , -0.66835 ,  0.26654 ,\n",
       "        2.842   ,  0.26844 , -0.59537 , -0.5004  ,  1.5199  ,  0.039641,\n",
       "        1.6659  ,  0.99758 , -0.5597  , -0.70493 , -0.0309  , -0.28302 ,\n",
       "       -0.13564 ,  0.6429  ,  0.41491 ,  1.2362  ,  0.76587 ,  0.97798 ,\n",
       "        0.58507 , -0.30176 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict={}\n",
    "with open('C:/Users/marci/Documents/Portfolio/TwitterClassification/TwitterStream/data/glove.6B.50d.txt','rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "embeddings_dict[b'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_closest_embeddings(embedding):\n",
    "   return sorted(embeddings_dict.keys(), key=lambda word:\n",
    "       spatial.distance.euclidean(embeddings_dict[word], embedding)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {\n",
    "    'videogame': 0,\n",
    "    'music': 1,\n",
    "    'city': 2,\n",
    "    'sport': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "videogame_words = find_closest_embeddings(embeddings_dict[b\"videogame\"])\n",
    "music_words = find_closest_embeddings(embeddings_dict[b\"music\"])\n",
    "city_words = find_closest_embeddings(embeddings_dict[b\"city\"])\n",
    "sport_words = find_closest_embeddings(embeddings_dict[b\"sport\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "videogame_similars = np.asarray(videogame_words[:500])\n",
    "music_similars = np.asarray(music_words[:500])\n",
    "city_similars = np.asarray(city_words[:500])\n",
    "sport_similars = np.asarray(sport_words[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,   2, 345, ...,  32,  21, 123])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 500)\n",
    "xU, xL = x + 500, x\n",
    "prob = ss.norm.cdf(xU, scale = 249) - ss.norm.cdf(xL, scale = 250)\n",
    "prob = prob / prob.sum() # normalize the probabilities so their sum is 1\n",
    "rand_arr = np.random.choice(x, size = 16*10000, p = prob)\n",
    "plt.hist(rand_arr, bins=len(x))\n",
    "rand_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artificial_phrase_matrix(seed):\n",
    "    np.random.seed(seed)\n",
    "    artificial_phrase_matrix = []\n",
    "    for i in range(10000):\n",
    "        phrase_size = np.random.randint(1,17)\n",
    "        artificial_phrase_matrix.append(np.random.choice(x, size = phrase_size, p = prob))\n",
    "    return artificial_phrase_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "videogame_apm = get_artificial_phrase_matrix(1)\n",
    "music_apm = get_artificial_phrase_matrix(2)\n",
    "city_apm = get_artificial_phrase_matrix(3)\n",
    "sport_apm = get_artificial_phrase_matrix(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_db = {\n",
    "    'videogame':{\n",
    "        'apm': videogame_apm, \n",
    "        'similar_words': videogame_similars\n",
    "        },\n",
    "    'music': {\n",
    "        'apm': music_apm,\n",
    "        'similar_words': music_similars\n",
    "        },\n",
    "    'city': {\n",
    "        'apm': city_apm,\n",
    "        'similar_words': city_similars\n",
    "        },\n",
    "    'sport': {\n",
    "        'apm': sport_apm,\n",
    "        'similar_words': sport_similars\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  videogame\n",
      "[array([b'licensees', b'alchemist', b'toonz', b'ics', b'snes', b'vpro'],\n",
      "      dtype='|S17'), array([b'then-new', b'vpro', b'pgp', b'ruggedized', b'vb', b'visio',\n",
      "       b'hobbyist', b'ps3'], dtype='|S17')]\n",
      "subject:  music\n",
      "[array([b'musician', b'soulful', b'new', b'dancing', b'trio', b'genre',\n",
      "       b\"'80s\", b'performed', b'dancing'], dtype='|S16'), array([b'composed', b'hip', b'featured', b'blues', b'musician'],\n",
      "      dtype='|S16')]\n",
      "subject:  city\n",
      "[array([b'suburbs', b'already', b'northwest', b'georgia', b'community',\n",
      "       b'cities', b'central', b'northeastern', b'part', b'heavily',\n",
      "       b'outskirts'], dtype='|S13'), array([b'including', b'center', b'newark', b'seen', b'downtown',\n",
      "       b'waterfront', b'queens', b'public', b'offices', b'harlem',\n",
      "       b'alone', b'northeast', b'chicago', b'school', b'today'],\n",
      "      dtype='|S13')]\n",
      "subject:  sport\n",
      "[array([b'once', b'youth', b'paralympic', b'sportscar', b'good', b'model',\n",
      "       b'motorcycle', b'here', b'doing', b'means', b'sports'],\n",
      "      dtype='|S13'), array([b'importantly', b'rarely', b'bicycle', b'good', b'sports',\n",
      "       b'athletics', b'competes', b'jockey', b'amateur', b'nordic',\n",
      "       b'baseball', b'example', b'youth', b'formula', b'neither'],\n",
      "      dtype='|S13')]\n"
     ]
    }
   ],
   "source": [
    "for subject_name in artificial_db.keys():\n",
    "    subject = artificial_db[subject_name]\n",
    "    sents = []\n",
    "    for idx_list in subject['apm']:\n",
    "        sent = subject['similar_words'][idx_list]\n",
    "        sents.append(sent)\n",
    "    subject['sentences'] = sents\n",
    "    print('subject: ', subject_name)\n",
    "    print(subject['sentences'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  videogame\n",
      "['licensees alchemist toonz ics snes vpro', 'then-new vpro pgp ruggedized vb visio hobbyist ps3']\n",
      "subject:  music\n",
      "[\"musician soulful new dancing trio genre '80s performed dancing\", 'composed hip featured blues musician']\n",
      "subject:  city\n",
      "['suburbs already northwest georgia community cities central northeastern part heavily outskirts', 'including center newark seen downtown waterfront queens public offices harlem alone northeast chicago school today']\n",
      "subject:  sport\n",
      "['once youth paralympic sportscar good model motorcycle here doing means sports', 'importantly rarely bicycle good sports athletics competes jockey amateur nordic baseball example youth formula neither']\n"
     ]
    }
   ],
   "source": [
    "for subject_name in artificial_db.keys():\n",
    "    subject = artificial_db[subject_name]\n",
    "    phrases = []\n",
    "    for byte_list in subject['sentences']:\n",
    "        word_list = list(map(lambda b: b.decode('UTF-8'), byte_list))\n",
    "        phrase = ' '.join(word_list)\n",
    "        phrases.append(phrase)\n",
    "    subject['phrases'] = phrases\n",
    "    print('subject: ', subject_name)\n",
    "    print(subject['phrases'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1537864080857649152</td>\n",
       "      <td>Happy birthday!!!!!</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1537860083668832256</td>\n",
       "      <td>Jeongin now joining the crew and his weapon is...</td>\n",
       "      <td>['Person', 'Musician', 'Music Genre']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537825048635097088</td>\n",
       "      <td>Interviewed more than 400 people and confirmed...</td>\n",
       "      <td>['Brand Vertical', 'Brand Category', 'Brand']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1537837652522651649</td>\n",
       "      <td>Ass torture !!  this is called ass torture, wh...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1537845005129326592</td>\n",
       "      <td>Need for help!! im a victim of domestic abuse ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1537864080857649152                                Happy birthday!!!!!   \n",
       "1  1537860083668832256  Jeongin now joining the crew and his weapon is...   \n",
       "2  1537825048635097088  Interviewed more than 400 people and confirmed...   \n",
       "3  1537837652522651649  Ass torture !!  this is called ass torture, wh...   \n",
       "4  1537845005129326592  Need for help!! im a victim of domestic abuse ...   \n",
       "\n",
       "                                     annotations  \n",
       "0                                             []  \n",
       "1          ['Person', 'Musician', 'Music Genre']  \n",
       "2  ['Brand Vertical', 'Brand Category', 'Brand']  \n",
       "3                                             []  \n",
       "4                                             []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets = pd.read_csv('C:/Users/marci/Documents/Portfolio/TwitterClassification/TwitterStream/data/random_tweets_cleaned.csv')\n",
    "random_tweets = random_tweets.dropna()\n",
    "random_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "random_tweets_phrases = list(random_tweets.text)\n",
    "for subject in artificial_db:\n",
    "    random_tweets_phrases += artificial_db[subject]['phrases']\n",
    "cv = cv.fit(random_tweets_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_name in artificial_db:\n",
    "    subject = artificial_db[subject_name]\n",
    "    subject['sparse_vectorized_phrases'] = cv.transform(subject['phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78480"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantidade de palavras encontradas\n",
    "len(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x78480 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 343978 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrixes = []\n",
    "for subject_name in targets:\n",
    "    matrixes.append(artificial_db[subject_name]['sparse_vectorized_phrases'])\n",
    "X = sparse.vstack(matrixes)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 3 3] (40000,)\n"
     ]
    }
   ],
   "source": [
    "target_values = []\n",
    "for subject_name, target in targets.items():\n",
    "    length = artificial_db[subject_name]['sparse_vectorized_phrases'].shape[0]\n",
    "    values = np.zeros(length, dtype=int) + target\n",
    "    target_values.append(values)\n",
    "y = np.concatenate(target_values, axis=0)\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 78480)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model with sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model with sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_penalty = 'l1'\n",
    "_solver = 'liblinear'\n",
    "clf_lr_1 = LogisticRegression(penalty=_penalty, solver=_solver)\n",
    "\n",
    "_penalty = 'l2'\n",
    "_solver = 'liblinear'\n",
    "clf_lr_2 = LogisticRegression(penalty=_penalty, solver=_solver)\n",
    "\n",
    "_loss = 'hinge'\n",
    "_penalty = 'l1'\n",
    "clf_sgd_1 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'perceptron'\n",
    "_penalty = 'l1'\n",
    "clf_sgd_2 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'hinge'\n",
    "_penalty = 'l2'\n",
    "clf_sgd_3 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'perceptron'\n",
    "_penalty = 'l2'\n",
    "clf_sgd_4 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "clfs = [clf_lr_1, clf_lr_2, clf_sgd_1, clf_sgd_2, clf_sgd_3, clf_sgd_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(clf):\n",
    "    \n",
    "    #training\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    \n",
    "    _scoring = {'Accuracy':'accuracy', 'log loss':'neg_log_loss'}\n",
    "    _scoring = {'Accuracy':'accuracy', 'f1 score':'f1_weighted'}\n",
    "    _cv = ShuffleSplit(n_splits=10, test_size=.25)\n",
    "    cv_results = cross_validate(clf, X, y, scoring=_scoring, cv=_cv)\n",
    "\n",
    "    #print(\"train time: %0.3fs\" % cv_results['fit_time'])\n",
    "    #print(\"score time:  %0.3fs\" % cv_results['score_time'])\n",
    "    scores = cv_results\n",
    "    print('\\nMeans:')\n",
    "    for metric in _scoring:\n",
    "        print(\"%s:   %0.3f\" % (metric, cv_results['test_'+metric].mean()))\n",
    "    \n",
    "    #print('\\nRaw results:')\n",
    "    #for metric in _scoring:\n",
    "    #    print(\"%s:   \" % metric, cv_results['test_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training: \n",
      "LogisticRegression(penalty='l1', solver='liblinear')\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.989\n",
      "f1 score:   0.989\n",
      "================================================================================\n",
      "Training: \n",
      "LogisticRegression(solver='liblinear')\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.996\n",
      "f1 score:   0.996\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(penalty='l1')\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.982\n",
      "f1 score:   0.982\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(loss='perceptron', penalty='l1')\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.965\n",
      "f1 score:   0.965\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier()\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.995\n",
      "f1 score:   0.995\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(loss='perceptron')\n",
      "\n",
      "Means:\n",
      "Accuracy:   0.991\n",
      "f1 score:   0.991\n"
     ]
    }
   ],
   "source": [
    "for clf in clfs:\n",
    "    evaluate_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x78480 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 63 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(random_tweets.text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1537864080857649152</td>\n",
       "      <td>Happy birthday!!!!!</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1537860083668832256</td>\n",
       "      <td>Jeongin now joining the crew and his weapon is...</td>\n",
       "      <td>[person, musician, music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537825048635097088</td>\n",
       "      <td>Interviewed more than 400 people and confirmed...</td>\n",
       "      <td>[brand vertical, brand category, brand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1537837652522651649</td>\n",
       "      <td>Ass torture !!  this is called ass torture, wh...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1537845005129326592</td>\n",
       "      <td>Need for help!! im a victim of domestic abuse ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130168</th>\n",
       "      <td>1537834573916086273</td>\n",
       "      <td>Kue lapis nct dream</td>\n",
       "      <td>[person, person, musician, musician, music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130169</th>\n",
       "      <td>1537854815635927040</td>\n",
       "      <td>My comfort in the chaos..    ||</td>\n",
       "      <td>[person, actor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130170</th>\n",
       "      <td>1537828257290407936</td>\n",
       "      <td>Guys, ini nct dream era apa? gils, their side ...</td>\n",
       "      <td>[person, person, musician, musician, music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130171</th>\n",
       "      <td>1537832359306792966</td>\n",
       "      <td>Ingat ttp slalu :  ,  ,</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130172</th>\n",
       "      <td>1537843973330374656</td>\n",
       "      <td>So excited to see beomgyu</td>\n",
       "      <td>[person, musician, music]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0       1537864080857649152   \n",
       "1       1537860083668832256   \n",
       "2       1537825048635097088   \n",
       "3       1537837652522651649   \n",
       "4       1537845005129326592   \n",
       "...                     ...   \n",
       "130168  1537834573916086273   \n",
       "130169  1537854815635927040   \n",
       "130170  1537828257290407936   \n",
       "130171  1537832359306792966   \n",
       "130172  1537843973330374656   \n",
       "\n",
       "                                                     text  \\\n",
       "0                                     Happy birthday!!!!!   \n",
       "1       Jeongin now joining the crew and his weapon is...   \n",
       "2       Interviewed more than 400 people and confirmed...   \n",
       "3       Ass torture !!  this is called ass torture, wh...   \n",
       "4       Need for help!! im a victim of domestic abuse ...   \n",
       "...                                                   ...   \n",
       "130168                                Kue lapis nct dream   \n",
       "130169                    My comfort in the chaos..    ||   \n",
       "130170  Guys, ini nct dream era apa? gils, their side ...   \n",
       "130171                            Ingat ttp slalu :  ,  ,   \n",
       "130172                          So excited to see beomgyu   \n",
       "\n",
       "                                        annotations  \n",
       "0                                                []  \n",
       "1                         [person, musician, music]  \n",
       "2           [brand vertical, brand category, brand]  \n",
       "3                                                []  \n",
       "4                                                []  \n",
       "...                                             ...  \n",
       "130168  [person, person, musician, musician, music]  \n",
       "130169                              [person, actor]  \n",
       "130170  [person, person, musician, musician, music]  \n",
       "130171                                           []  \n",
       "130172                    [person, musician, music]  \n",
       "\n",
       "[130173 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets['annotations'] = random_tweets['annotations'].str.lower()\n",
    "random_tweets['annotations'] = random_tweets['annotations'].str.replace('music genre', 'music')\n",
    "random_tweets['annotations'] = random_tweets['annotations'].str.replace('video game', 'videogame')\n",
    "random_tweets['annotations'] = random_tweets['annotations'].str.replace('cities', 'city')\n",
    "random_tweets['annotations'] = random_tweets['annotations'].apply(ast.literal_eval)\n",
    "random_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actor', 'american football game', 'animals', 'athlete',\n",
       "       'award show', 'baseball game', 'basketball game', 'book',\n",
       "       'book genre'], dtype='<U32')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_names = np.unique(np.concatenate(random_tweets['annotations']))\n",
    "annotation_names[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 28233),\n",
       " ('musician', 9619),\n",
       " ('music', 7988),\n",
       " ('brand vertical', 6076),\n",
       " ('brand category', 10960)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_frequency = Counter(np.concatenate(random_tweets['annotations']))\n",
    "list(annot_frequency.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_least_frequenty_annotation(x):\n",
    "    if (len(x)==0):\n",
    "        return 'Unkown'\n",
    "    least_freq = x[0]\n",
    "    for name in x[1:]:\n",
    "        if (annot_frequency[name] < annot_frequency[least_freq]):\n",
    "            least_freq = name\n",
    "    return least_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweets['annotations'] = random_tweets['annotations'].apply(get_least_frequenty_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1537860083668832256</td>\n",
       "      <td>Jeongin now joining the crew and his weapon is...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1537843356767621122</td>\n",
       "      <td>Choeaedol giveaway ! follow  like and rt  drop...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1537853213411627009</td>\n",
       "      <td>Jeon jungkook was embarrassed to do the ending...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1537842564048494593</td>\n",
       "      <td>Roses noodles and doughnuts ed frost and daugh...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1537859576145772544</td>\n",
       "      <td>Presence on the global stage means that the wo...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "1   1537860083668832256  Jeongin now joining the crew and his weapon is...   \n",
       "13  1537843356767621122  Choeaedol giveaway ! follow  like and rt  drop...   \n",
       "38  1537853213411627009  Jeon jungkook was embarrassed to do the ending...   \n",
       "41  1537842564048494593  Roses noodles and doughnuts ed frost and daugh...   \n",
       "42  1537859576145772544  Presence on the global stage means that the wo...   \n",
       "\n",
       "   annotations  \n",
       "1        music  \n",
       "13       music  \n",
       "38       music  \n",
       "41       music  \n",
       "42       music  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test = random_tweets[random_tweets['annotations'].isin(targets)]\n",
    "base_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'music': 3972, 'city': 1037, 'videogame': 1265, 'sport': 278})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(base_test['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_8152\\2192653802.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  base_test['annotations'] = base_test['annotations'].replace(targets)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 3972, 2: 1037, 0: 1265, 3: 278})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test['annotations'] = base_test['annotations'].replace(targets)\n",
    "Counter(base_test['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1537860083668832256</td>\n",
       "      <td>Jeongin now joining the crew and his weapon is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1537843356767621122</td>\n",
       "      <td>Choeaedol giveaway ! follow  like and rt  drop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1537853213411627009</td>\n",
       "      <td>Jeon jungkook was embarrassed to do the ending...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1537842564048494593</td>\n",
       "      <td>Roses noodles and doughnuts ed frost and daugh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1537859576145772544</td>\n",
       "      <td>Presence on the global stage means that the wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130134</th>\n",
       "      <td>1537826952878465024</td>\n",
       "      <td>I don't think our next live will be in 2 years...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130164</th>\n",
       "      <td>1537846137620959233</td>\n",
       "      <td>Choeadol &amp;gt; 600 hearts &amp;gt; follow me &amp;amp; ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130168</th>\n",
       "      <td>1537834573916086273</td>\n",
       "      <td>Kue lapis nct dream</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130170</th>\n",
       "      <td>1537828257290407936</td>\n",
       "      <td>Guys, ini nct dream era apa? gils, their side ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130172</th>\n",
       "      <td>1537843973330374656</td>\n",
       "      <td>So excited to see beomgyu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6552 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "1       1537860083668832256   \n",
       "13      1537843356767621122   \n",
       "38      1537853213411627009   \n",
       "41      1537842564048494593   \n",
       "42      1537859576145772544   \n",
       "...                     ...   \n",
       "130134  1537826952878465024   \n",
       "130164  1537846137620959233   \n",
       "130168  1537834573916086273   \n",
       "130170  1537828257290407936   \n",
       "130172  1537843973330374656   \n",
       "\n",
       "                                                     text  annotations  \n",
       "1       Jeongin now joining the crew and his weapon is...            1  \n",
       "13      Choeaedol giveaway ! follow  like and rt  drop...            1  \n",
       "38      Jeon jungkook was embarrassed to do the ending...            1  \n",
       "41      Roses noodles and doughnuts ed frost and daugh...            1  \n",
       "42      Presence on the global stage means that the wo...            1  \n",
       "...                                                   ...          ...  \n",
       "130134  I don't think our next live will be in 2 years...            1  \n",
       "130164  Choeadol &gt; 600 hearts &gt; follow me &amp; ...            1  \n",
       "130168                                Kue lapis nct dream            1  \n",
       "130170  Guys, ini nct dream era apa? gils, their side ...            1  \n",
       "130172                          So excited to see beomgyu            1  \n",
       "\n",
       "[6552 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_penalty = 'l1'\n",
    "_solver = 'liblinear'\n",
    "clf_lr_1 = LogisticRegression(penalty=_penalty, solver=_solver)\n",
    "\n",
    "_penalty = 'l2'\n",
    "_solver = 'liblinear'\n",
    "clf_lr_2 = LogisticRegression(penalty=_penalty, solver=_solver)\n",
    "\n",
    "_loss = 'hinge'\n",
    "_penalty = 'l1'\n",
    "clf_sgd_1 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'perceptron'\n",
    "_penalty = 'l1'\n",
    "clf_sgd_2 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'hinge'\n",
    "_penalty = 'l2'\n",
    "clf_sgd_3 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "_loss = 'perceptron'\n",
    "_penalty = 'l2'\n",
    "clf_sgd_4 = SGDClassifier(loss=_loss, penalty=_penalty)\n",
    "\n",
    "clfs = [clf_lr_1, clf_lr_2, clf_sgd_1, clf_sgd_2, clf_sgd_3, clf_sgd_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,_,y_train,_ = train_test_split(X,y,test_size=.01,shuffle=True)\n",
    "X_test = cv.transform(base_test['text'])\n",
    "y_test = base_test['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(clf):\n",
    "    \n",
    "    #training\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    \n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    fit_time = time() - t0\n",
    "    \n",
    "    print(\"train time: %0.3fs\" % fit_time)\n",
    "    \n",
    "    t0 = time()\n",
    "    prediction = clf.predict(X_test)\n",
    "    score_time = time() - t0\n",
    "    \n",
    "    print(\"score time:  %0.3fs\" % score_time)\n",
    "    \n",
    "    print('\\nRaw results:')\n",
    "    print(\"Accuracy:  %.3f\" % metrics.accuracy_score(y_test, prediction))\n",
    "    print(\"f1 weighted score:  %.3f\" % metrics.f1_score(y_test, prediction, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training: \n",
      "LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train time: 0.477s\n",
      "score time:  0.002s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.231\n",
      "f1 weighted score:  0.187\n",
      "================================================================================\n",
      "Training: \n",
      "LogisticRegression(solver='liblinear')\n",
      "train time: 0.503s\n",
      "score time:  0.002s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.230\n",
      "f1 weighted score:  0.184\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(penalty='l1')\n",
      "train time: 0.170s\n",
      "score time:  0.002s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.254\n",
      "f1 weighted score:  0.228\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(loss='perceptron', penalty='l1')\n",
      "train time: 0.187s\n",
      "score time:  0.002s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.372\n",
      "f1 weighted score:  0.392\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier()\n",
      "train time: 0.097s\n",
      "score time:  0.002s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.237\n",
      "f1 weighted score:  0.196\n",
      "================================================================================\n",
      "Training: \n",
      "SGDClassifier(loss='perceptron')\n",
      "train time: 0.101s\n",
      "score time:  0.003s\n",
      "\n",
      "Raw results:\n",
      "Accuracy:  0.292\n",
      "f1 weighted score:  0.278\n"
     ]
    }
   ],
   "source": [
    "for clf in clfs:\n",
    "    test_model(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ea365085f3807110e1cdcb04122fb278047fe100a09bc2730588139759355b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
